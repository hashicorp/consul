---
layout: docs
page_title: Deploy Consul on kind
description: >-
  Deploy Consul locally on kind and learn how to manage your Consul datacenter with the Consul CLI, UI, and API. Finally, configure Consul service mesh for services in your Kubernetes cluster.
---

# Deploy Consul on kind

In this tutorial, you will create a local Kubernetes cluster with `kind`, then deploy a Consul datacenter to your `kind` cluster with HashiCorp‚Äôs official Helm chart or the Consul K8S CLI. After deploying Consul, you will interact with Consul using the CLI, UI, and/or API. You will then deploy two services that use Consul to discover and communicate with each other.

## Prerequisites

For this tutorial, you will need:

- [kind >= 0.17.0](https://kind.sigs.k8s.io/docs/user/quick-start/)
- [kubectl >= 1.23](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
- [docker >= 20.0](https://docs.docker.com/get-docker/)
- [helm >= 3.6](https://helm.sh/docs/using_helm/)
- [consul >= 1.15.1](/consul/install/)

## Create a Kind cluster

With `kind` you can quickly create a local Kubernetes cluster. By default, `kind` names your cluster "kind", but you may name it anything you like by specifying the `--name` option. This tutorial assumes the cluster is named `dc1`. Refer to the [kind documentation](https://kind.sigs.k8s.io/docs/user/quick-start/#setting-kubernetes-version) for information about how to specify additional parameters using a yaml configuration file.

```shell-session
$ kind create cluster --name dc1
```

The output will be similar to the following.

```plaintext hideClipboard
Creating cluster "dc1" ...
 ‚úì Ensuring node image (kindest/node:v1.25.1) üñº
 ‚úì Preparing nodes üì¶
 ‚úì Writing configuration üìú
 ‚úì Starting control-plane üïπÔ∏è
 ‚úì Installing CNI üîå
 ‚úì Installing StorageClass üíæ
Set kubectl context to "kind-dc1"
You can now use your cluster with:

kubectl cluster-info --context kind-dc1

Have a nice day! üëã
```

## Deploy Consul

You can deploy a complete Consul datacenter using the official Consul Helm chart or the Consul K8S CLI. The chart comes with reasonable defaults, however, you will override a few values to integrate more easily with `kind` and enable useful features. You can review the Consul Kubernetes installation [documentation](/consul/docs/k8s/installation/install) to learn more about these installation options.

### Create a values file

To customize your deployment, create a `values.yaml` file to customization your Consul deployment.

<CodeBlockConfig filename="values.yaml">

```yaml
# Contains values that affect multiple components of the chart.
global:
  # The main enabled/disabled setting.
  # If true, servers, clients, Consul DNS and the Consul UI will be enabled.
  enabled: true
  # The prefix used for all resources created in the Helm chart.
  name: consul
  # The name of the datacenter that the agents should register as.
  datacenter: dc1
  # Enables TLS across the cluster to verify authenticity of the Consul servers and clients.
  tls:
    enabled: true
  # Enables ACLs across the cluster to secure access to data and APIs.
  acls:
    # If true, automatically manage ACL tokens and policies for all Consul components.
    manageSystemACLs: true
# Configures values that configure the Consul server cluster.
server:
  enabled: true
  # The number of server agents to run. This determines the fault tolerance of the cluster.
  replicas: 1
# Contains values that configure the Consul UI.
ui:
  enabled: true
  # Registers a Kubernetes Service for the Consul UI as a NodePort.
  service:
    type: NodePort
```

</CodeBlockConfig>

### Install Consul in your cluster

You can now deploy a complete Consul datacenter in your Kubernetes cluster using the official Consul Helm chart or the Consul K8S CLI.

<Tabs>
<Tab heading="Consul K8S CLI (Mac & Linux)">

```shell-session
$ brew tap hashicorp/tap
```

```shell-session
$ brew install hashicorp/tap/consul-k8s
```

```shell-session
$ consul-k8s install -config-file=values.yaml -set global.image=hashicorp/consul:1.15.1
```

Confirm the installation details when prompted by the installer.

```plaintext hideClipboard
...
Proceed with installation? (y/N) y
```

<Note>

 You can review the official [Consul K8S CLI documentation](/consul/docs/k8s/k8s-cli) to learn more about additional settings.

</Note>

</Tab>
<Tab heading="Helm">

```shell-session
$ helm repo add hashicorp https://helm.releases.hashicorp.com
"hashicorp" has been added to your repositories
```

```shell-session
$ helm install --values values.yaml consul hashicorp/consul --create-namespace --namespace consul --version "1.1.0"
```

<Note>

 You can review the official [Helm chart values](/consul/docs/k8s/helm#configuration-values) to learn more about the default settings.

</Note>

</Tab>
</Tabs>

Run the command `kubectl get pods` to verify the Consul resources were successfully created.

```shell-session
$ kubectl get pods --namespace consul
NAME                                           READY   STATUS    RESTARTS   AGE
consul-connect-injector-6fc8d669b8-2n82l       1/1     Running   0          2m34s
consul-server-0                                1/1     Running   0          2m34s
consul-webhook-cert-manager-64889c4964-wxc9b   1/1     Running   0          2m34s
```

## Configure your CLI to interact with Consul cluster

In this section, you will set environment variables in your terminal so your Consul CLI can interact with your Consul cluster. The Consul CLI reads these environment variables for behavior defaults and will reference these values when you run `consul` commands.

Tokens are artifacts in the ACL system used to authenticate users, services, and Consul agents. Since ACLs are enabled in this Consul datacenter, entities requesting access to a resource must include a token that is linked with a policy, service identity, or node identity that grants permission to the resource. The ACL system checks the token and grants or denies access to resources based on the associated permissions. A bootstrap token has unrestricted privileges to all resources and APIs.

Retrieve the ACL bootstrap token from the respective Kubernetes secret and set it as an environment variable.

```shell-session
$ export CONSUL_HTTP_TOKEN=$(kubectl get --namespace consul secrets/consul-bootstrap-acl-token --template={{.data.token}} | base64 -d)
```

Set the Consul destination address. By default, Consul runs on port `8500` for `http` and `8501` for `https`.

```shell-session
$ export CONSUL_HTTP_ADDR=https://127.0.0.1:8501
```

Remove SSL verification checks to simplify communication to your Consul cluster.

```shell-session
$ export CONSUL_HTTP_SSL_VERIFY=false
```

<Note>

 In a production environment, we recommend keeping this SSL verification set to `true`. Only remove this verification for if you have a Consul cluster without TLS configured in development environment and demonstration purposes.

</Note>

## View Consul services

In this section, you will view your Consul services with the CLI, UI, and/or API to explore the details of your service mesh.

<Tabs>
<Tab heading="CLI">

Open a separate terminal window and expose the Consul server with `kubectl port-forward` using the `consul-ui` service name as the target. 

```shell-session
$ kubectl port-forward svc/consul-ui --namespace consul 8501:443
```

In your original terminal, run the CLI command `consul catalog services` to return the list of services registered in Consul. Notice this returns only the `consul` service since it is the only running service in your Consul cluster.

```shell-session
$ consul catalog services
consul
```

Agents run in either server or client mode. Server agents store all state information, including service and node IP addresses, health checks, and configuration. Client agents are lightweight processes that make up the majority of the datacenter. They report service health status to the server agents. Clients must run on every pod where services are running.

Run the CLI command `consul members` to return the list of Consul agents in your environment. 

```shell-session
$ consul members
Node                Address           Status  Type    Build   Protocol  DC   Partition  Segment
consul-server-0     10.244.0.12:8301  alive   server  1.14.0  2         dc1  default    <all>
```

</Tab>
<Tab heading="UI">

Output the token value to your terminal and copy the value to your clipboard. You will use this ACL token to authenticate in the Consul UI.

```shell-session
$ echo $CONSUL_HTTP_TOKEN
fe0dd5c3-f2e1-81e8-cde8-49d26cee5efc
```

Open a separate terminal window and expose the Consul UI with `kubectl port-forward` using the `consul-ui` service name as the target. 

```shell-session
$ kubectl port-forward svc/consul-ui --namespace consul 8501:443
```

Open [https://localhost:8501](https://localhost:8501) in your browser to find the Consul UI. Since this environment uses a self-signed TLS certificate for its resources, click to proceed through the certificate warnings.

On the left navigation pane, click **Services** to review your deployed services. At this time, you will only find the `consul` service.

![Consul UI Services Page](/img/consul/kubernetes-gs-deploy_consul_ui_services.png 'Consul services page in UI with Consul services')

By default, the anonymous ACL policy allows you to view the contents of Consul services, nodes, and intentions. To make changes and see more details within the Consul UI, click **Log In** in the top right and insert your bootstrap ACL token.

![Consul UI Login Page](/img/consul/kubernetes-gs-deploy_consul_ui_login.png 'Consul login page in UI')

After successfully authenticating with your ACL token, you are now able to view additional Consul components and make changes in the UI. Notice you can view and manage more options under the **Access Controls** section on the left navigation pane.

![Consul UI Post Authentication](/img/consul/kubernetes-gs-deploy_consul_ui_post_authentication.png 'Consul UI post authentication')

On the left navigation pane, click on **Nodes**. 

Agents run in either server or client mode. Server agents store all state information, including service and node IP addresses, health checks, and configuration. Client agents are lightweight processes that make up the majority of the datacenter. They report service health status to the server agents. Clients must run on every pod where services are running.
 
![Consul UI Post Authentication](/img/consul/kubernetes-gs-deploy_consul_view_nodes.png 'Consul UI post authentication')

</Tab>
<Tab heading="API">

Open a separate terminal window and expose the Consul server with `kubectl port-forward` using the `consul-ui` service name as the target. 

```shell-session
$ kubectl port-forward svc/consul-ui --namespace consul 8501:443
```

In your original terminal, view the list of services registered in Consul.

```shell-session
$ curl -k \
    --header "X-Consul-Token: $CONSUL_HTTP_TOKEN" \
    $CONSUL_HTTP_ADDR/v1/catalog/services
```

Sample output: 

```json hideClipboard
{"consul":[]}
```

Agents run in either server or client mode. Server agents store all state information, including service and node IP addresses, health checks, and configuration. Client agents are lightweight processes that make up the majority of the datacenter. They report service health status to the server agents. Clients must run on every pod where services are running.

View the list of server and client Consul agents in your environment.

```shell-session
$ curl -k \
    --header "X-Consul-Token: $CONSUL_HTTP_TOKEN" \
    $CONSUL_HTTP_ADDR/v1/agent/members\?pretty
```

Sample output:

```json hideClipboard
[
    {
        "Name": "consul-server-0",
        "Addr": "10.244.0.13",
        "Port": 8301,
        "Tags": {
            "acls": "1",
            "bootstrap": "1",
            "build": "1.14.0",
            "dc": "dc1",
            "ft_fs": "1",
            "ft_si": "1",
            "grpc_port": "8502",
            "id": "8016fc4d-767f-8552-b018-0812228bd135",
            "port": "8300",
            "raft_vsn": "3",
            "role": "consul",
            "segment": "",
            "use_tls": "1",
            "vsn": "2",
            "vsn_max": "3",
            "vsn_min": "2",
            "wan_join_port": "8302"
        },
        "Status": 1,
        "ProtocolMin": 1,
        "ProtocolMax": 5,
        "ProtocolCur": 2,
        "DelegateMin": 2,
        "DelegateMax": 5,
        "DelegateCur": 4
    }
]
```

</Tab>
</Tabs>

All services listed in your Consul catalog are empowered with Consul's service discovery capabilities that simplify scalability challenges and improve application resiliency. Review the [Service Discovery overview](/consul/docs/concepts/service-discovery) page to learn more.

## Deploy services into your service mesh

Now that you have a running Consul service mesh, you can deploy services to it.

### Deploy two demo services

You will now deploy a two-tier application made of a backend data service that
returns a number (the `counting` service), and a frontend `dashboard` that pulls
from the `counting` service over HTTP and displays the number.

Create a deployment definition, service, and service account for the `counting`
service named `counting.yaml`.

<CodeBlockConfig filename="counting.yaml">

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: counting
  namespace: default
automountServiceAccountToken: true
---
apiVersion: v1
kind: Service
metadata:
  name: counting
  namespace: default
  labels:
    app: counting
spec:
  type: ClusterIP
  ports:
  - port: 9001
    targetPort: 9001
  selector:
    app: counting
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: counting
  name: counting
spec:
  replicas: 1
  selector:
    matchLabels:
      app: counting
  template:
    metadata:
      annotations:
        consul.hashicorp.com/connect-inject: 'true'
      labels:
        app: counting
    spec:
      serviceAccountName: counting
      containers:
      - name: counting
        image: hashicorp/counting-service:0.0.2
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9001
```

</CodeBlockConfig>

Create a deployment definition, service, and service account for
the `dashboard` service named `dashboard.yaml`.

<CodeBlockConfig filename="dashboard.yaml">

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard
  namespace: default
automountServiceAccountToken: true
---
apiVersion: v1
kind: Service
metadata:
  name: dashboard
  namespace: default
  labels:
    app: dashboard
spec:
  type: ClusterIP
  ports:
  - port: 9002
    targetPort: 9002
  selector:
    app: dashboard
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: dashboard
  name: dashboard
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dashboard
  template:
    metadata:
      annotations:
        consul.hashicorp.com/connect-inject: 'true'
        consul.hashicorp.com/connect-service-upstreams: 'counting:9001'
      labels:
        app: dashboard
    spec:
      serviceAccountName: dashboard
      containers:
      - name: dashboard
        image: hashicorp/dashboard-service:0.0.4
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9002
        env:
        - name: COUNTING_SERVICE_URL
          value: 'http://localhost:9001'
```

</CodeBlockConfig>

Use `kubectl` to deploy the counting and dashboard services.

```shell-session
$ kubectl apply -f counting.yaml && kubectl apply -f dashboard.yaml
serviceaccount/counting created
service/counting created
deployment.apps/counting created
serviceaccount/dashboard created
service/dashboard created
deployment.apps/dashboard created
```

To verify the services were deployed, run `kubectl get pods` until you see both services are ready or refresh the Consul UI until you observe that the `counting` and `dashboard` services are running. 

![Counting and Dashboard Services in Consul UI](/img/consul/kubernetes-dashboard_service-consul-ui.png 'Counting and Dashboard Services in Consul UI')

## Test the demo application

Open a separate terminal window and expose the `dashboard` UI with `kubectl port-forward` using the `dashboard` service name as the target.

```shell-session
$ kubectl port-forward svc/dashboard --namespace default 9002:9002
```

Open [http://localhost:9002](http://localhost:9002) in your browser. Notice that the service will display a message that the "Counting Service is Unreachable", and the count will display as "-1". This is expected behavior as `dashboard` cannot reach the `counting` backend since you have not defined any intentions yet.

![Dashboard UI Connection Error](/img/consul/kubernetes-dashboard_service-home_page_unreachable.png 'Dashboard UI Connection Error')

## Create intentions

To see how intentions affect communication between the services in your service mesh, you will create intentions following the "least-privilege" principle that allow communication between your services.

Create a file named `intentions.yaml` to define intentions that allow the `dashboard` service to communicate with the `counting` service. 

<CodeBlockConfig filename="intentions.yaml" hideClipboard>

```yml
apiVersion: consul.hashicorp.com/v1alpha1
kind: ServiceIntentions
metadata:
  name: dashboard-to-counting
spec:
  destination:
    name: counting
  sources:
    - name: dashboard
      action: allow
```

</CodeBlockConfig>

Deploy the service intentions to allow the HashiCups services to interact with each other..

```shell-session
$ kubectl apply --filename intentions.yaml
```

### Confirm applied intentions

Open a separate terminal window and expose the `dashboard` UI with `kubectl port-forward` using the `dashboard` service name as the target.

```shell-session
$ kubectl port-forward svc/dashboard --namespace default 9002:9002
```

Check out the `dashboard` UI at [http://localhost:9002](http://localhost:9002). Refresh the page and notice that the application is now fully functional. It will display the `dashboard` UI with a number retrieved from the `counting` service using Consul service discovery and service mesh functionality.

![Dashboard UI Connection Success](/img/consul/kubernetes-dashboard_service-home_page_success.png 'Dashboard UI Connection Success')

## Clean up

Run `kind delete cluster` to clean up your local demo environment.

```shell-session
$ kind delete cluster --name dc1
Deleting cluster "dc1" ...
```

## Next steps

In this tutorial, you deployed a Consul datacenter onto a `kind` cluster. After deploying Consul, you interacted with Consul using the CLI, UI, and API. Finally, you deployed two services that use Consul to discover and communicate with each other.

Feel free to explore these tutorials and collections to learn more about Consul
service mesh, microservices, and Kubernetes security.

- [Get Started with Consul on Kubernetes](/consul/tutorials/get-started-kubernetes)
- [Consul Docs](/consul/docs/)
- [Consul Kubernetes Deployment Guide](/consul/tutorials/kubernetes/kubernetes-deployment-guide)
- [Consul Kubernetes Security](/consul/tutorials/kubernetes/kubernetes-secure-agents)
