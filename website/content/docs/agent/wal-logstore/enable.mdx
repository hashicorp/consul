---
layout: docs
page_title: Enable the experimental WAL LogStore backend
description: >-
  Learn how to safely configure and test the experimental WAL backend in your Consul deployment.
---

# Enable the experimental WAL LogStore backend

This topic describes how to safely configure and test the WAL backend in your Consul deployment.

The overall process for enabling the WAL LogStore backend for one server consists of the following steps. You will need to repeat these steps for all Consul servers.

1. Enable log verification.
1. Select target server to enable WAL.
1. Stop target server gracefully.
1. Remove data directory from target server.
1. Update target server's configuration.
1. Start target server.
1. Monitor target server raft metrics and logs.

!> **Upgrade warning:** The WAL LogStore backend is experimental.

## Requirements

- All servers in the Datacenter should be upgraded to Consul 1.15 using the [standard upgrade procedure](/consul/docs/upgrading/general-process) and the [1.15 upgrade notes](/consul/docs/upgrading/upgrade-specific#consul-1-15-x).
- You need a Consul cluster with at least 3 nodes to safely test the new backend without downtime.

In addition, we recommend:

- Taking a snapshot prior to testing in case things don't go to plan.
- Monitoring Consul server metrics and logs, and setting an alert on specific log events occurring.
- Enabling WAL in a pre-production environment and leave it running for a period of time (few days or weeks) before enabling it in production.

## Risks

Although this document describes configuring and testing the WAL backend in a way that limits risk, there still are potential risks:

 - If WAL is enabled on a server and is found to corrupt data in some way. That server's data can't be recovered. The server will need to be restarted with an empty data directory and reload it's state from the leader.
 - It's possible that WAL might corrupt data or otherwise have a bug that causes the server to panic and crash. It may even not be able to restart if the same bug occurs when it reads from the logs on startup. In this case as above the server would need to be restarted with WAL disabled and an empty data directory.
 - It's _possible_ though unlikely that if WAL corrupted data, clients might read corrupted data from that server. For example invalid IP addresses or have tokens fail to match. This is unlikely even if there is a WAL corruption bug hit because replication typically takes place using in-memory cached objects rather than reads from disk. The issue would be fixed by restoring the server.
 - If you enable a server to use WAL using Consul OSS or on a voting server with Consul Enterprise, it's _possible_ that the WAL could cause corruption of that server's state (with the caveats above) _and then_ become the leader and replicate that corruption to all other servers. In this scenario only a restore from backup would recover a completely un-corrupt state. - If you test on a non-voting server in Enterprise, this can't happen.


## Enable log verification

You must enable log verification on any voting server in Enterprise and all servers in OSS since the leader writes verification checkpoints.

On each voting server, add the following to the server's configuration file:

```hcl
raft_logstore {
  verification {
    enabled = true
    interval = "60s"
  }
}
```

Restart each server to apply the changes (`consul reload` is not sufficient). Wait for each one to become a healthy voter again using `consul operator raft list-peers` before moving on to the next. This can take a few minutes if the snapshot is large.

Your log entries on the servers be similar to the following:

```log hideClipboard
2023-01-31T14:44:31.174Z [INFO]  agent.server.raft.logstore.verifier: verification checksum OK: elapsed=488.463268ms leaderChecksum=f15db83976f2328c rangeEnd=357802 rangeStart=298132 readChecksum=f15db83976f2328c
```

## Select target server to enable WAL

If you are using Consul OSS or Consul Enterprise without non-voting servers, select one of the follower servers. As noted in [Risks](#risks), Consul Enterprise users with non-voting servers should select a non-voting server at first. 

Retrieve the current state of the servers by running the following:

```shell-session
$ consul operator raft list-peers
```

## Stop target server

Stop the target server gracefully. For example, if you are using `systemcmd`, 
run the following command:

```shell-session
$ systemctl stop consul
```

If you have any configuration management automation that might interfere with this process (for example, Chef or Puppet), you must disable them until you have completely enabled WAL as a storage backend.

## Remove data directory from target server

Temporarily moving the data directory to a different location is less destructive than deleting it. We recommend doing this in case you unsuccessfully enable WAL. However, once you restart the server, you should not use the old data directory (`/data-dir/raft.bak`) for recovery, and will eventually need to delete it.

Move the data directory. Replace `/data-dir` the value you have specified in your configuration file.

```shell-session
$ mv /data-dir/raft /data-dir/raft.bak
```

When switching backend, you must always remove the _whole raft directory_ not just the `raft.db` file or `wal` directory since the log must always be consistent with the snapshots to avoid undefined behavior or data loss.

## Update target server's configuration

Add the following to the target server's configuration file:

```hcl
raft_logstore {
  backend = "wal"
  verification {
    enabled = true
    interval = "60s"
  }
}
```

## Start target server

Start the target server. For example, if you are using `systemcmd`, run the following command:

```shell-session
$ systemctl start consul
```

Watch for the server to become a healthy voter again.

```shell-session
$ consul operator raft list-peers
```

## Monitor target server Raft metrics and logs

See the section below on [monitoring WAL tests](/consul/docs/agent/wal-logstore/monitoring).

We recommend you leave the cluster in this configuration for days or weeks, assuming that you see no negative metrics or verification errors in logs to increase confidence in the WAL under varying workloads and during routine server restarts etc.

If you disabled Chef, Puppet or similar earlier, you may want to consider enabling it again while the test runs. Ensure that it will not "fix" the Consul configuration file and remove the different backend though.

## Next steps

- If you see any verification errors, performance anomalies or other suspicious behavior from the target server during the test, you should follow [the procedure to revert back to BoltDB](/consul/docs/agent/wal-logstore/revert-to-boltdb).

- If you see no errors and would like to expand the test further, you can repeat the above procedure on another target server. We suggest waiting a while after each and slowly rolling out. Once the majority of your servers are using WAL any bugs not yet found could result in cluster unavailability.

- If you wish to permanently enable `wal` on all servers, you'll need to follow the above steps on each one. Even if `backend = "wal"` is set in logs, servers will continue to use BoltDB if they find an existing raft.db file in the data dir.