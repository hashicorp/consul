---
layout: docs
page_title: Enterprise to CE downgrade Process
description: >-
  Guide to downgrade from consul enterprise to consul ce
---

## Introduction

This document describes some best practices that you should follow when
downgrading from Consul Enterprise to Consul CE.

## Download the CE Version

First, download the binary for the CE version you want.

<Tabs>
<Tab heading="Binary">

All current and past versions of the CE and Enterprise releases are
available here:

- https://releases.hashicorp.com/consul

</Tab>
<Tab heading="Docker">

Docker containers are available at this location:

- **CE:** https://hub.docker.com/r/hashicorp/consul

</Tab>

</Tabs>

## Prepare for the Downgrade to CE

**1.** Take a snapshot:

```
consul snapshot save backup.snap
```

You can inspect the snapshot to ensure if was successful with:

```
consul snapshot inspect backup.snap
```

Example output:

```
ID           2-1182-1542056499724
Size         4115
Index        1182
Term         2
Version      1
```

This will ensure you have a safe fallback option in case something goes wrong. Store
this snapshot somewhere safe. More documentation on snapshot usage is available here:

- [consul.io/commands/snapshot](/consul/commands/snapshot)
- [Backup Consul Data and State tutorial](/consul/tutorials/production-deploy/backup-and-restore)

**2.** Temporarily modify your Consul configuration so that its [log_level](/consul/docs/agent/config/cli-flags#_log_level)
is set to `debug`. After doing this, issue the following command on your servers to
reload the configuration:

```
consul reload
```

This change will give you more information to work with in the event something goes wrong.


**3.** Remove references to tenanted services from service-resolver, service-splitter and service-router config entries.


## Perform the Downgrade

**1.** Issue the following command to discover which server is currently the leader:

```
consul operator raft list-peers
```

You should receive output similar to this (exact formatting and content may differ based on version):

```
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```
Take note of which agent is the leader.

**2.** Restart or redeploy all clients with a CE version of the binary. You can use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the  agent manually.

**3.** Update server binaries to use the CE version

**4.** The following steps must be done in order on the server agents, leaving the leader
agent for last. First, an env var named `CONSUL_ENTERPRISE_DOWNGRADE_TO_CE` should be set to `true` on the servers. Then use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the agent manually.

To validate that the agent has rejoined the cluster and is in sync with the leader, issue the
following command:

```
consul info
```

Check whether the `commit_index` and `last_log_index` fields have the same value. If done properly,
this should avoid an unexpected leadership election due to loss of quorum.

**5.** Double-check that all servers are showing up in the cluster as expected and are on
the correct version by issuing:

```
consul members
```

You should receive output similar to this:

```
Node       Address         Status  Type    Build  Protocol  DC
dc1-node1  10.11.0.2:8301  alive   server  1.8.3  2         dc1
dc1-node2  10.11.0.3:8301  alive   server  1.8.3  2         dc1
dc1-node3  10.11.0.4:8301  alive   server  1.8.3  2         dc1
```

Also double-check the raft state to make sure there is a leader and sufficient voters:

```
consul operator raft list-peers
```

You should receive output similar to this:

```
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```

**6.** Set your `log_level` back to its original value and issue the following command
on your servers to reload the configuration:

```
consul reload
```

## What happens during the downgrade

During the downgrade as the raft replication logs start flowing into the ce server we handle them in one of the following ways:

- Drop the request

   For example :
   - Register requests - register requests having non default namespace, having services or health checks in non default namespace or partition are dropped.

   - Peering Write Requests - peering write requests are dropped if the local partiton connecting to the peer is non default. 

- Filter out tenanted data from the request

  For example :
  - Intention Sources - Intention sources targeting non default namespaces or partitions are filtered out of the config entry.
  - Exported Services - Exports of services within non default namespaces or partitions are filtered out of the config entry.

- Panic and stop the downgrade

  Config Entries regarding traffic routing cannot be safely filtered because Consul cannot know whether the resulting filtered config entries send traffic towards services which can handle that traffic. In these situations, you are required to first remove references to services within non default namespaces or partitions from within those config entries. When encountering these config entries Consul CE in the downgrade mode will panic in order to not cause harm to existing service mesh routing.
  
  For example :
  - Service Splitter,Service Resolver,Service Router Config Entry requests - if these config entries have references to tenanted services it will cause a panic.