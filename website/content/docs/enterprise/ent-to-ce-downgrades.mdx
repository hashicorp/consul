---
layout: docs
page_title: Downgrade from Consul Enterprise to the community edition
description: >-
  Learn how to downgrade your installation of Consul Enterprise to the free Consul community edition (CE)
---

# Downgrade from Consul Enterprise to the community edition

This document describes how to downgrade from Consul Enterprise to Consul community edition (CE).This is only applicable from version 1.18 onwards for both Consul Enterprise and Consul community edition (CE).

## Download the CE binary version

First, download the binary for the CE version(>=1.18).

<Tabs>
<Tab heading="Binary">

All current and past versions of the CE and Enterprise releases are
available on the [HashiCorp releases page](https://releases.hashicorp.com/consul)

Example:
```shell-session
export VERSION=1.18.0
curl https://releases.hashicorp.com/consul/${VERSION}/consul_${VERSION} _linux_amd64.zip
```

</Tab>
<Tab heading="Kubernetes">

To downgrade consul version on k8s we will have to modify the image version in your helm chart similar to the upgrade process mentioned in this doc.
  [Upgrade Consul version](https://developer.hashicorp.com/consul/docs/k8s/upgrade#upgrade-consul-version)

</Tab>

</Tabs>

## Prepare for the downgrade to CE

**1.** Take a snapshot:

```
consul snapshot save backup.snap
```

You can inspect the snapshot to ensure if was successful with:

```
consul snapshot inspect backup.snap
```

Example output:

```
ID           2-1182-1542056499724
Size         4115
Index        1182
Term         2
Version      1
```

This will ensure you have a safe fallback option in case something goes wrong. Store
this snapshot somewhere safe. More documentation on snapshot usage is available here:

- [Consul snapshot](/consul/commands/snapshot)
- [Backup Consul Data and State tutorial](/consul/tutorials/production-deploy/backup-and-restore)

**2.** Temporarily modify your Consul configuration so that its [log_level](/consul/docs/agent/config/cli-flags#_log_level)
       is set to `debug`. After doing this, issue the following command on your servers to
       reload the configuration:

```
$ consul reload
```

This change will give you more information to work with in the event something goes wrong.

**3.** Modify the following config entries if applicable: 

[Service resolver](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-resolver): 

  1. Remove services configured as failovers in non default namespace or  belonging to a sameness group.

  2. Remove services configured as redirects belonging to non default namespaces or partiton.

[Service splitter](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-splitter):

  1. Remove services configured as splits belonging to non default namespaces or partiton

[Service router](https://developer.hashicorp.com/consul/docs/connect/config-entries/service-router):

  1. Remove services configured as destination belonging to non default namespaces or partiton

## Perform the Downgrade

**1.** Restart or redeploy all clients with a CE version of the binary. You can use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the  agent manually.

 Example :- 
   ```shell-session
   sudo systemctl restart consul
   ```

**2.** Issue the following command to discover which server is currently the leader:

```shell-session
consul operator raft list-peers
```

You should receive output similar to this (exact formatting and content may differ based on version):

```shell-session
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```
Take note of which agent is the leader.



**3.** Update server binaries to use the CE version.The following steps must be done in order on the server agents, leaving the leader agent for last.

 - An env var named `CONSUL_ENTERPRISE_DOWNGRADE_TO_CE` should be set to `true` on the servers. 

   Following is an example to set the env var if you are using `systemd` :
   1. Edit the Consul systemd service unit file
      ```shell-session
      sudo vi /etc/systemd/system/consul.service
      ```
   2. Add the environment variables you want to set for Consul under the [Service] section of the unit file and save the file
      ```shell-session
      [Service]
       Environment=CONSUL_ENTERPRISE_DOWNGRADE_TO_CE=true
      ```
   3. Reload systemd
      ```shell-session
      sudo systemctl daemon-reload
      ```
    
 - Use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the agent manually.

   Example :- 
   ```shell-session
 $  sudo systemctl restart consul
   ```

 - To validate that the agent has rejoined the cluster and is in sync with the leader, issue the following command:

      ```shell-session
      $ consul info
      ```

   Check whether the `commit_index` and `last_log_index` fields have the same value. If done properly,
   this should avoid an unexpected leadership election due to loss of quorum.

**4.** Double-check that all servers are showing up in the cluster as expected and are on 
the correct version by issuing:

```shell-session
$ consul members
```

You should receive output similar to this:

```shell-session
Node       Address         Status  Type    Build  Protocol  DC
dc1-node1  10.11.0.2:8301  alive   server  1.18.0  2         dc1
dc1-node2  10.11.0.3:8301  alive   server  1.18.0  2         dc1
dc1-node3  10.11.0.4:8301  alive   server  1.18.0  2         dc1
```

Also double-check the raft state to make sure there is a leader and sufficient voters:

```shell-session
consul operator raft list-peers
```

You should receive output similar to this:

```shell-session
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```

**5.** Set your `log_level` back to its original value and issue the following command
on your servers to reload the configuration:

```shell-session
$ consul reload
```

## Downgrade process details

During the downgrade process, the Consul CE server handles the raft replication logs in one of the following ways:

- Drops the request. 
- Filters out data from requests sent from non-default namespaces or partitions
- Panics and stops the downgrade 

Following are some examples where the server may drop the requests: 

- Registration requests in non-default namespace and services or health checks in non-default namespaces or partitions.

- Write requests to peered clusters if the local partiton connecting to the peer is non-default. 

Following are some examples where the server may filter out data from requests:

- Intention sources that target non-default namespaces or partitions are filtered out of the configuration entry.
- Exports of services within non-default namespaces or partitions are filtered out of the configuration entry.

The server may panic and stop the downgrade when configuration entries that route traffic cannot be safely filtered. This is because Consul cannot know whether the resulting filtered configuration entries send traffic to services that can handle the traffic. Consul CE panics in order to prevent harm to existing service mesh routes.

In these situations, you must first remove references to services within non-default namespaces or partitions from those configuration entries. 
  
The server may panic in the following cases:

- Service splitter, service resolver, and service router configuration entry requests that have references to services located in non-default namespaces or partitions cause the server to panic.