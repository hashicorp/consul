---
layout: docs
page_title: Enterprise to CE downgrade Process
description: >-
  Guide to downgrade from consul enterprise to consul ce
---

## Introduction

This document describes how to downgrade from Consul Enterprise to Consul community edition (CE).

## Download the CE binary version

First, download the binary for the CE version you want.

<Tabs>
<Tab heading="Binary">

All current and past versions of the CE and Enterprise releases are
available on the [HashiCorp releases page](https://releases.hashicorp.com/consul)

</Tab>
<Tab heading="Docker">

Docker containers are available on [Docker Hub](https://hub.docker.com/r/hashicorp/consul)

</Tab>

</Tabs>

## Prepare for the downgrade to CE

**1.** Take a snapshot:

```
consul snapshot save backup.snap
```

1. Inspect the snapshot to ensure that Consul successfully captured it using the following command:

    ```
    consul snapshot inspect backup.snap
    ```

    Example output:

    ```
    ID           2-1182-1542056499724
    Size         4115
    Index        1182
    Term         2
    Version      1
    ```

    Inspecting the snapshot ensures that you have a safe fallback option in case something goes wrong. Store
this snapshot somewhere safe. Refer to the following documentation for additional information about creating and using snapshots:

    - [Consul Snapshot](/consul/commands/snapshot)
    - [Backup Consul Data and State tutorial](/consul/tutorials/production-deploy/backup-and-restore)

**2.** Temporarily modify your Consul configuration so that its [log_level](/consul/docs/agent/config/cli-flags#_log_level)
is set to `debug`. After doing this, issue the following command on your servers to
reload the configuration:

```
consul reload
```

This change will give you more information to work with in the event something goes wrong.


**3.** Remove references to tenanted services from service-resolver, service-splitter and service-router config entries.


## Perform the Downgrade

**1.** Issue the following command to discover which server is currently the leader:

```
consul operator raft list-peers
```

You should receive output similar to this (exact formatting and content may differ based on version):

```
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```
Take note of which agent is the leader.

**2.** Restart or redeploy all clients with a CE version of the binary. You can use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the  agent manually.

**3.** Update server binaries to use the CE version

**4.** The following steps must be done in order on the server agents, leaving the leader
agent for last. First, an env var named `CONSUL_ENTERPRISE_DOWNGRADE_TO_CE` should be set to `true` on the servers. Then use a service management system (e.g., systemd, upstart, etc.) to restart the Consul service. If you are not using a service management system, you must restart the agent manually.

To validate that the agent has rejoined the cluster and is in sync with the leader, issue the
following command:

```
consul info
```

Check whether the `commit_index` and `last_log_index` fields have the same value. If done properly,
this should avoid an unexpected leadership election due to loss of quorum.

**5.** Double-check that all servers are showing up in the cluster as expected and are on
the correct version by issuing:

```
consul members
```

You should receive output similar to this:

```
Node       Address         Status  Type    Build  Protocol  DC
dc1-node1  10.11.0.2:8301  alive   server  1.8.3  2         dc1
dc1-node2  10.11.0.3:8301  alive   server  1.8.3  2         dc1
dc1-node3  10.11.0.4:8301  alive   server  1.8.3  2         dc1
```

Also double-check the raft state to make sure there is a leader and sufficient voters:

```
consul operator raft list-peers
```

You should receive output similar to this:

```
Node       ID                                    Address         State     Voter  RaftProtocol
dc1-node1  ae15858f-7f5f-4dcb-b7d5-710fdcdd2745  10.11.0.2:8300  leader    true   3
dc1-node2  20e6be1b-f1cb-4aab-929f-f7d2d43d9a96  10.11.0.3:8300  follower  true   3
dc1-node3  658c343b-8769-431f-a71a-236f9dbb17b3  10.11.0.4:8300  follower  true   3
```

**6.** Set your `log_level` back to its original value and issue the following command
on your servers to reload the configuration:

```
consul reload
```

## Downgrade process details

During the downgrade process, the Consul CE server handles the raft replication logs in one of the following ways:

- Drops the request. 
- Filters out data from requests sent from non-default namespaces and partitions
- Panics and stops the downgrade 

The server may drop the following types of requests: 

- Registration requests in non-default namespace and services or health checks in non-default namespaces or partitions.

- Write requests to peered clusters if the local partiton connecting to the peer is non-default. 

The server may filter out data from requests to non-default namespaces and partitions in the following cases:

- Intention sources that target non-default namespaces or partitions are filtered out of the configuration entry.
- Exports of services within non-default namespaces or partitions are filtered out of the configuration entry.

The server may panic and stop the downgrade when configuration entries that route traffic cannot be safely filtered. This is because Consul cannot know whether the resulting filtered configuration entries send traffic to services that can handle the traffic. Consul CE panics in order to prevent harm to existing service mesh routes.

In these situations, you must first remove references to services within non-default namespaces or partitions from those configuration entries. 
  
The server may panic in the following cases:

- Service splitter, service resolver, and service router configuration entry requests that have references to services located in non-default namespaces and partitions cause the server to panic.