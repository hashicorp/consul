---
layout: docs
page_title: Service Mesh Observability - Service Dashboard
description: >-
  This documentation provides an overview of the Service Dashboard.
---

# Service Dashboard

The **Service Dashboard** offers an overview of the performance and health of individual services within the Consul service mesh. It provides insights into service availability, request success rates, latency, and connection metrics. This dashboard is essential for maintaining optimal service performance and quickly identifying any issues with service communications.

## Metrics Overview:

### Total Running Services
- **Metric:** `sum(envoy_server_live{app!="traffic-generator"})`
- **Description:** This gauge tracks the total number of running services within the mesh that are not labeled as `traffic-generator`. It provides an overall view of active services, helping operators maintain visibility into service availability.

### Total Request Success Rate
- **Metric:** `sum(irate(envoy_cluster_upstream_rq_xx{envoy_response_code_class!="5", envoy_response_code_class!="4", consul_destination_service=~"$service"}[10m])) / sum(irate(envoy_cluster_upstream_rq_xx{consul_destination_service=~"$service"}[10m]))`
- **Description:** This stat visualizes the success rate of upstream requests to the selected service. It filters out 4xx and 5xx response codes, providing a clearer picture of how well the service is performing in terms of handling requests successfully.

### Total Failed Request Rate
- **Metric:** `sum(irate(envoy_cluster_upstream_rq_xx{envoy_response_code_class=~"4|5", consul_destination_service=~"$service"}[10m])) / sum(irate(envoy_cluster_upstream_rq_xx{consul_destination_service=~"$service"}[10m]))`
- **Description:** This stat tracks the rate of failed requests (4xx and 5xx errors) for the selected service. It helps operators quickly identify if there are issues with client requests or server errors for a specific service.

### Average Request Response Time in Milliseconds
- **Metric:** `sum(rate(envoy_cluster_upstream_rq_time_sum{consul_destination_service=~"$service"}[10m])) / sum(rate(envoy_cluster_upstream_rq_total{consul_destination_service=~"$service"}[10m]))`
- **Description:** This gauge displays the average response time for requests to the selected service, providing an overview of the service's performance and responsiveness.

### Total Failed Requests
- **Metric:** `sum(increase(envoy_cluster_upstream_rq_xx{envoy_response_code_class=~"4|5", consul_destination_service=~"$service"}[10m])) by(local_cluster)`
- **Description:** This gauge tracks the total number of failed requests over a 10-minute window, categorized by service. It allows for easy identification of services that are experiencing high failure rates.

### Dataplane Latency: 
- **Metrics:**
  - `p50`: `histogram_quantile(0.50, sum by(le) (rate(envoy_cluster_upstream_rq_time_bucket{kubernetes_namespace=~"$namespace", local_cluster=~"$service"}[5m])))`
  - `p75`: `histogram_quantile(0.75, sum by(le) (rate(envoy_cluster_upstream_rq_time_bucket{kubernetes_namespace=~"$namespace", local_cluster=~"$service"}[5m])))`
  - `p90`: `histogram_quantile(0.90, sum by(le) (rate(envoy_cluster_upstream_rq_time_bucket{kubernetes_namespace=~"$namespace", local_cluster=~"$service"}[5m])))`
  - `p99.9`: `histogram_quantile(0.999, sum by(le) (rate(envoy_cluster_upstream_rq_time_bucket{kubernetes_namespace=~"$namespace", local_cluster=~"$service"}[5m])))`
- **Description:** This stat tracks the dataplane latency percentiles (p50, p75, p90, p99.9) for the selected service. It gives detailed insights into the distribution of latency within the service's request handling, helping identify performance bottlenecks.

### Total TCP Inbound and Outbound Bytes
- **Metric:** `sum(rate(envoy_tcp_downstream_cx_rx_bytes_total{}[10m])) by (local_cluster)`
- **Description:** This time series shows the total number of inbound and outbound TCP bytes for services within the mesh. It provides visibility into the data transfer patterns and volume between services.

### Total TCP Inbound and Outbound Bytes Buffered
- **Metric:** `sum(rate(envoy_tcp_downstream_cx_rx_bytes_buffered{}[10m])) by (local_cluster)`
- **Description:** This metric tracks the amount of TCP traffic buffered during inbound and outbound communications. It helps in identifying whether there is any potential latency caused by buffer overflow or congestion.

### Total TCP Downstream Active Connections
- **Metric:** `sum(rate(envoy_tcp_downstream_cx_total{}[10m])) by(local_cluster)`
- **Description:** This metric counts the total number of active TCP downstream connections, providing an overview of the current connection load on the services within the mesh.

### Total Active HTTP Upstream Connections
- **Metric:** `sum(envoy_cluster_upstream_cx_active{app=~"$service"}) by (app)`
- **Description:** This time series tracks the total number of active HTTP upstream connections for the selected service. It helps monitor connection patterns and assess load.

### Total Active HTTP Downstream Connections
- **Metric:** `sum(envoy_http_downstream_cx_active{app=~"$service"}) by (app)`
- **Description:** This time series monitors the number of active HTTP downstream connections for the selected service, providing visibility into the current active user or client load on the service.

### Upstream Requests by Status Code
- **Metric:** `sum by(namespace,app,envoy_response_code_class) (rate(envoy_cluster_upstream_rq_xx[5m]))`
- **Description:** This metric tracks the number of upstream requests, grouped by HTTP status codes (e.g., 2xx, 4xx, 5xx), giving insight into the health of the requests being made to upstream services from the selected service.

### Downstream Requests by Status Code
- **Metric:** `sum(rate(envoy_http_downstream_rq_xx{envoy_http_conn_manager_prefix="public_listener"}[5m])) by (namespace, app, envoy_response_code_class)`
- **Description:** This time series tracks downstream HTTP requests by status code, showing how well the selected service is responding to downstream requests from clients.

### Connections Rejected
- **Metrics:**
  - **Overload Rejections:** `rate(envoy_listener_downstream_cx_overload_reject{}[$__interval])`
  - **Overflow Rejections:** `rate(envoy_listener_downstream_global_cx_overflow{}[$__interval])`
- **Description:** This metric tracks the number of connections rejected due to overload or overflow conditions on listeners. Monitoring these values helps identify if the service is under too much load or has insufficient capacity to handle the incoming connections.

## Customization Options:

The **Service Dashboard** offers various customization options to help operators focus on specific services and metrics. By using these options, operators can tailor the dashboard to their needs and improve their ability to monitor and troubleshoot service health.

- **Filter by Service:** You can filter the dashboard by the service you want to monitor. This helps narrow down the metrics to the service of interest and provides a more targeted view of its performance.
  
- **Filter by Namespace:** The namespace filter allows operators to focus on a particular namespace in a multi-tenant or multi-namespace environment, isolating the service metrics within that specific context.

- **Time Range Selection:** The dashboard supports flexible time range selection, allowing operators to analyze service behavior over different time periods. This is helpful for pinpointing issues that may occur at specific times or during high-traffic periods.

- **Percentile Latency Tracking:** The dashboard allows operators to track multiple latency percentiles (p50, p75, p90, p99.9) to get a more detailed view of how the service performs across different levels of traffic load.
